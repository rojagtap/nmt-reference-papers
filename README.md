# nmt-reference-papers
Some research papers on major advances in Neural Machine Translations, feel free to send pull requests to add more.

## Contents:
1. vanishing_gradient- On the difficulty of training Recurrent Neural Networks (Vanishing Gradient).
2. seq2seq- Sequence to Sequence Learning with Neural Networks.
3. bleu- BLEU: a Method for Automatic Evaluation of Machine Translation.
4. bahadanau- Neural Machine Translation by jointly learning to align and translate (Attention).
5. self_attention_1- Long Short-Term Memory-Networks for Machine Reading.
6. self_attention_2- A Decomposable Attention Model for Natural Language Inference.
7. self_attention_3- A structured Self-Attentive sentence embedding.
8. self_attention_4- A deep Reinforced Model for Abstractive Summarization.
9. transformer- Attention Is All You Need.
10. convseq- Convolutional Sequence to Sequence Learning.
11. cnn- Gradient-Based Learning Applied to Document Recognition.
12. resnet- Deep Residual Learning for Image Recognition.
13. effective_attn- Effective Approaches to Attention-based Neural Machine Translation.
14. GAN- Generative Adversarial Nets.
15. SA_GAN- Self-Attention Generative Adversarial Networks.
16. rogue1- ROUGE: A Package for Automatic Evaluation of Summaries.
17. pointer_softmax- Pointing the Unknown Words.
18. nmt_architectures- Massive Exploration of Neural Machine Translation Architectures.
19. gnmt- Googleâ€™s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation.
20. glue- GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.
21. bert1- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
22. bert2- On the use of BERT for Neural Machine Translation.
23. bert3- Towards Making the Most of BERT in Neural Machine Translation.
